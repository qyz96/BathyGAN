{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import Data_prep as dp\n",
    "from scipy.io import savemat, loadmat\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.layers import Reshape, Lambda, Input, Dense, Flatten, Conv2D, Conv2DTranspose\n",
    "from keras.layers import Activation, ZeroPadding2D, BatchNormalization, MaxPooling2D, AveragePooling2D, UpSampling2D\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "K.set_image_data_format('channels_last')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from keras import backend\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from matplotlib import pyplot\n",
    "from keras.constraints import Constraint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    \n",
    "    max_=np.max(data, axis=0)\n",
    "    min_=np.min(data, axis=0)\n",
    "    mean_=np.mean(data,axis=0)\n",
    "    std_=np.std(data,axis=0)\n",
    "    \n",
    "    return np.divide((data-mean_),(std_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-height",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.math.multiply(y_true, y_pred))\n",
    "\n",
    "class ClipConstraint(Constraint):\n",
    "    # set clip value when initialized\n",
    "    def __init__(self, clip_value):\n",
    "        self.clip_value = clip_value\n",
    "\n",
    "    # clip model weights to hypercube\n",
    "    def __call__(self, weights):\n",
    "        return backend.clip(weights, -self.clip_value, self.clip_value)\n",
    "\n",
    "    # get the config\n",
    "    def get_config(self):\n",
    "        return {'clip_value': self.clip_value}\n",
    "\n",
    "def generator(latent_dim):\n",
    "\n",
    "    #decoder network:\n",
    "    l_decode=0.0\n",
    "    #input/BC:\n",
    "    latent_input = Input(shape=(latent_dim,), name='latent_decode') #input\n",
    "    x = latent_input\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    #fully connected layer 1:\n",
    "    x = Dense(1000, kernel_initializer=init, kernel_regularizer = regularizers.l1(l_decode))(x)\n",
    "    x = BatchNormalization(axis = 1)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    #fully connected layer 2:\n",
    "    x = Dense(5000, kernel_initializer=init, kernel_regularizer = regularizers.l1(l_decode))(x)\n",
    "    x = BatchNormalization(axis = 1)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    #reshaping:\n",
    "    x = Reshape((10, 125, 4))(x)\n",
    "\n",
    "    #CNN/upsampling layer 1:\n",
    "    x = Conv2DTranspose(4, (5, 5), strides = (1, 1), padding='same', kernel_initializer=init, kernel_regularizer = regularizers.l1(l_decode))(x)\n",
    "    x = BatchNormalization(axis = 3)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = UpSampling2D((2, 2), interpolation = 'bilinear')(x)\n",
    "    #x = AveragePooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2DTranspose(4, (5, 5), strides = (1, 1), padding='same', kernel_initializer=init, kernel_regularizer = regularizers.l1(l_decode))(x)\n",
    "    x = BatchNormalization(axis = 3)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = UpSampling2D((2, 2), interpolation = 'bilinear')(x)\n",
    "    #x = AveragePooling2D((2, 2), padding='same')(x)\n",
    "    #output:\n",
    "#     x = Conv2D(1, (9, 9), padding='same', strides = (4, 4), kernel_initializer=init)(x)\n",
    "#     x = BatchNormalization(axis = 3)(x)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "    outputs = Conv2DTranspose(1, (9, 9), padding='same', strides = (1, 1), kernel_initializer=init, activation = 'tanh', name='bathymetry')(x)\n",
    "    #outputs = UpSampling2D((2, 2))(x)\n",
    "    #set decoder model:\n",
    "    decoder = Model([latent_input], outputs, name='decoder')\n",
    "    return decoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-sussex",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def critic(in_shape=(40, 500, 1)):\n",
    "\n",
    "    #decoder network:\n",
    "    l_decode=0.0\n",
    "    #input/BC:\n",
    "    inputs = Input(shape=in_shape) #input\n",
    "    const = ClipConstraint(100000)\n",
    "    init = RandomNormal(stddev=0.1)\n",
    "    y = inputs\n",
    "    y = Conv2D(128, (5, 5), padding='same', strides=(2, 2))(y)\n",
    "    y = BatchNormalization(axis = 3)(y)\n",
    "    y = LeakyReLU(alpha=0.5)(y)\n",
    "    y = MaxPooling2D((2, 2), padding='same')(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = Conv2D(128, (5, 5), padding='same', strides=(2, 2))(y)\n",
    "    y = BatchNormalization(axis = 3)(y)\n",
    "    y = LeakyReLU(alpha=0.5)(y)\n",
    "    y = MaxPooling2D((2, 2), padding='same')(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = Conv2D(128, (5, 5), padding='same', strides=(2, 2))(y)\n",
    "    y = BatchNormalization(axis = 3)(y)\n",
    "    y = LeakyReLU(alpha=0.5)(y)\n",
    "    y = MaxPooling2D((2, 2), padding='same')(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(1024)(y)\n",
    "    y = BatchNormalization(axis = 1)(y)\n",
    "    y = LeakyReLU(alpha=0.2)(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(512)(y)\n",
    "    y = BatchNormalization(axis = 1)(y)\n",
    "    y = LeakyReLU(alpha=0.2)(y)\n",
    "    outputs = Dense(1, activation = 'sigmoid')(y)\n",
    "    #y = BatchNormalization(axis = 1)(y)\n",
    "    #outputs = LeakyReLU(alpha=0.2)(y)\n",
    "    #set critic model:\n",
    "    critic = Model([inputs], [outputs])\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    critic.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return critic \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-frederick",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\td_model.trainable = False\n",
    "\t# get noise and label inputs from generator model\n",
    "\tgen_noise = g_model.input\n",
    "\t# get image output from the generator model\n",
    "\tgen_output = g_model.output\n",
    "\t# connect image output and label input from generator as inputs to discriminator\n",
    "\tgan_output = d_model(gen_output)\n",
    "\t# define gan model as taking noise and label and outputting a classification\n",
    "\tmodel = Model([gen_noise], [gan_output])\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# split into images and labels\n",
    "\timages = dataset\n",
    "\t# choose random instances\n",
    "\tix = randint(0, images.shape[0], n_samples)\n",
    "\t# select images and labels\n",
    "\tX = images[ix]\n",
    "\t# generate class labels\n",
    "\ty = ones((n_samples, 1))\n",
    "\treturn [X], y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    #x_input = tf.random.normal(latent_dim * n_samples)\n",
    "    z_input = tf.random.normal([n_samples, latent_dim]) \n",
    "    return [z_input]\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    images = generator.predict([z_input])\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return [images], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128, n_steps = 4):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    d_loss1 = 0 \n",
    "    d_loss2 = 0\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            real_iter = 1\n",
    "            if (d_loss1 > 0):\n",
    "                real_iter = 1\n",
    "            fake_iter = 1\n",
    "            if (d_loss2 > 0):\n",
    "                real_iter = 1\n",
    "            for k in range(real_iter):\n",
    "                # get randomly selected 'real' samples\n",
    "                X_real, y_real = generate_real_samples(dataset, n_batch)\n",
    "                # update discriminator model weights\n",
    "                d_loss1 = d_model.train_on_batch([X_real], y_real)\n",
    "                # generate 'fake' examples\n",
    "            for kk in range(real_iter):\n",
    "                [X_fake], y_fake = generate_fake_samples(g_model, latent_dim, n_batch)\n",
    "                # update discriminator model weights\n",
    "                d_loss2 = d_model.train_on_batch([X_fake], y_fake)\n",
    "                # prepare points in latent space as input for the generator\n",
    "                [z_input] = generate_latent_points(latent_dim, n_batch)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch([z_input], [y_gan])\n",
    "            ####\n",
    "            d_loss1 = d_model.test_on_batch([X_real], y_real)\n",
    "            d_loss2 = d_model.test_on_batch([X_fake], y_fake)\n",
    "            g_loss = gan_model.test_on_batch([z_input], [y_gan])\n",
    "            ####\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %(i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_c(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128, n_steps = 4):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    d_loss1 = 0 \n",
    "    d_loss2 = 0\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            print(i + j)\n",
    "            real_iter = 1\n",
    "            if (d_loss1 > 0):\n",
    "                real_iter = 1\n",
    "            fake_iter = 1\n",
    "            if (d_loss2 > 0):\n",
    "                real_iter = 1\n",
    "            for k in range(real_iter):\n",
    "                # get randomly selected 'real' samples\n",
    "                X_real, y_real = generate_real_samples(dataset, n_batch)\n",
    "                # update discriminator model weights\n",
    "                d_loss1 = d_model.train_on_batch([X_real], y_real)\n",
    "            # generate 'fake' examples\n",
    "            for kk in range(real_iter):\n",
    "                [X_fake], y_fake = generate_fake_samples(g_model, latent_dim, n_batch)\n",
    "                # update discriminator model weights\n",
    "                d_loss2 = d_model.train_on_batch([X_fake], y_fake)\n",
    "            # prepare points in latent space as input for the generator\n",
    "            [z_input] = generate_latent_points(latent_dim, n_batch)\n",
    "            # create inverted labels for the fake samples\n",
    "            ####\n",
    "            d_loss1 = d_model.test_on_batch([X_real], y_real)\n",
    "            d_loss2 = d_model.test_on_batch([X_fake], y_fake)\n",
    "            ####\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d1=%.3f, d2=%.3f' %(i+1, j+1, bat_per_epo, d_loss1, d_loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_g(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128, n_steps = 4):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # prepare points in latent space as input for the generator\n",
    "            [z_input] = generate_latent_points(latent_dim, n_batch)\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            # create inverted labels for the fake samples\n",
    "            g_loss = gan_model.train_on_batch([z_input], [y_gan])\n",
    "            ####\n",
    "            g_loss = gan_model.test_on_batch([z_input], [y_gan])\n",
    "            ####\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d1=%.3f' %(i+1, j+1, bat_per_epo, g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('Sav_BC_func_PCGA_1std_noise10_std4_zmax94_48_12_s0_5_6.mat')\n",
    "vel = data['velocity_prof'] #velocities\n",
    "z_f = data['z_f'] #free surface elevation\n",
    "Q_b = data['Q_b'] #influx\n",
    "Z = data['Z'] #bathymetry\n",
    "vx, vy, _ = dp.xy_vel_sep(vel) #v_x/v_y separation\n",
    "data_z = np.reshape(Z.T, (4500, 41, 501, 1))\n",
    "data_z = data_z[:, :40, :500, :]\n",
    "std_ = np.reshape(np.std(np.reshape(data_z, (4500, 20000)), axis = 0), (1,40,500,1))\n",
    "mean_ = np.reshape(np.mean(np.reshape(data_z, (4500, 20000)), axis = 0), (1,40,500,1))\n",
    "min_ = np.reshape(np.min(np.reshape(data_z, (4500, 20000)), axis = 0), (1,40,500,1))\n",
    "max_ = np.reshape(np.max(np.reshape(data_z, (4500, 20000)), axis = 0), (1,40,500,1))\n",
    "data_zn = ((data_z - min_) / (max_ - min_))\n",
    "data_zn = (data_zn - 0.5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = generator(50)\n",
    "c_model = critic()\n",
    "gan_model = define_gan(g_model, c_model)\n",
    "#train(g_model, c_model, gan_model, data_zn, 50, n_epochs=100, n_batch=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "epo = 100\n",
    "for k in range(epo):\n",
    "    #g_model = generator(50)\n",
    "    num_sample = data_zn.shape[0]\n",
    "    fake_samples = generate_fake_samples(g_model, 50, num_sample)[0][0]\n",
    "    x_train = np.concatenate((fake_samples, data_zn))\n",
    "    y_train = np.concatenate((np.zeros((num_sample, 1)), (np.ones((num_sample, 1)))))\n",
    "    r_index = np.random.permutation(num_sample * 2) - 1\n",
    "    x_train = x_train[r_index, :, :, :]\n",
    "    y_train = y_train[r_index, :]\n",
    "    n_epoch = 1\n",
    "    for j in range(n_epoch):\n",
    "        for i in range(282):\n",
    "            c_model.train_on_batch(x_train[i * 16 : (i + 1) * 16], y_train[i * 16 : (i + 1) * 16])\n",
    "            c_model.evaluate(x_train[i * 16 : (i + 1) * 16], y_train[i * 16 : (i + 1) * 16])\n",
    "    train_g(g_model, c_model, gan_model, data_zn, 50, n_epochs=2, n_batch=128, n_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model = critic()\n",
    "opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "c_model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "c_model.fit(x_train, y_train, validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model.fit(x_train, y_train, validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model.fit(data_zn, np.ones((4500, 1)), validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model.fit(fake_samples, np.zeros((4500, 1)), validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = generate_real_samples(data_zn, 10)[0][0]\n",
    "#plt.figure(figsize=(40,4))\n",
    "plt.imshow(im[0,:,:,0], aspect='auto', vmin = -1, vmax = 1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = generate_fake_samples(g_model, 50, 10)[0][0]\n",
    "im = ((im / 2) + 0.5) * (max_ - min_) + min_\n",
    "plt.figure(figsize=(50,5))\n",
    "plt.imshow(im[0,:,:,0], aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = generate_fake_samples(g_model, 50, 10)[0][0]\n",
    "im = ((im / 2) + 0.5) * (max_ - min_) + min_\n",
    "fake_river = np.reshape(Z.T, (4500, 41, 501, 1))[0, :, :, 0]\n",
    "fake_river[:40, :500] = im[0,:,:,0]\n",
    "dp.plt_im_tri(depth=fake_river.ravel(), fig_title = 'fake_river', fig_name = 'fake_river_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = generate_real_samples(data_zn, 10)[0][0]\n",
    "im = ((im / 2) + 0.5) * (max_ - min_) + min_\n",
    "fake_river = np.reshape(Z.T, (4500, 41, 501, 1))[0, :, :, 0]\n",
    "fake_river[:40, :500] = im[0,:,:,0]\n",
    "dp.plt_im_tri(depth=fake_river.ravel(), fig_title = 'real_river', fig_name = 'real_river_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model.save_weights('Bathy_Generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model.save_weights('Bathy_Critic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-printing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model(x_train[7000:7001,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(input_param):\n",
    "    \"\"\"\n",
    "    sampling the latent space from a Gaussian distribution:\n",
    "\n",
    "    # Input\n",
    "        input_param: mean and log of variance of q(z|x)\n",
    "\n",
    "    # Output\n",
    "        z: sampled latent space vector\n",
    "    \"\"\"\n",
    "    \n",
    "    #mean and log(var):\n",
    "    z_mean, z_log_var = input_param \n",
    "    \n",
    "    #dimensions:\n",
    "    dim_1 = K.shape(z_mean)[0]\n",
    "    dim_2 = K.int_shape(z_mean)[1]\n",
    "    \n",
    "    #sampling:\n",
    "    norm_sample = K.random_normal(shape=(dim_1, dim_2))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * norm_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(input_param):\n",
    "    \"\"\"\n",
    "    sampling the latent space from a Gaussian distribution:\n",
    "\n",
    "    # Input\n",
    "        input_param: mean and log of variance of q(z|x)\n",
    "\n",
    "    # Output\n",
    "        z: sampled latent space vector\n",
    "    \"\"\"\n",
    "    \n",
    "    #mean and log(var):\n",
    "    z_mean, z_log_var = input_param \n",
    "    \n",
    "    #dimensions:\n",
    "    dim_1 = K.shape(z_mean)[0]\n",
    "    dim_2 = K.int_shape(z_mean)[1]\n",
    "    \n",
    "    #sampling:\n",
    "    norm_sample = K.random_normal(shape=(dim_1, dim_2))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * norm_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder network:\n",
    "\n",
    "#regularization coefficient:\n",
    "l_encode=0.0\n",
    "\n",
    "#input:\n",
    "bathy_input = Input(shape=(41,501, 1), name = 'velocity')\n",
    "\n",
    "#CNN/pooling layer 1:\n",
    "x = Conv2D(2, (3, 3), strides=1, padding='same',kernel_regularizer = regularizers.l2(l_encode))(bathy_input)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "#CNN/pooling layer 2:\n",
    "x = Conv2D(4, (3, 3), strides=1, padding='same',kernel_regularizer = regularizers.l2(l_encode))(x)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "#flattening:\n",
    "shape = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "#fully connected layer 1:\n",
    "x = Dense(500,kernel_regularizer = regularizers.l2(l_encode))(x)\n",
    "x = BatchNormalization(axis = 1)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#output:\n",
    "z_mean = Dense(50,activation='sigmoid',name='z_mean')(x)\n",
    "z_log_var = Dense(50,activation='sigmoid',name='z_log_var')(x)\n",
    "z = Lambda(sampling, output_shape=(50,), name='latent_encode')([z_mean, z_log_var])\n",
    "\n",
    "#set encoder model:\n",
    "encoder = Model(bathy_input, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder network:\n",
    "\n",
    "#regularization coefficient\n",
    "l_decode=0.0\n",
    "\n",
    "#input/BC:\n",
    "BC_input = Input(shape=(2, ), name = 'BC') #BC\n",
    "latent_input = Input(shape=(50,), name='latent_decode') #input\n",
    "x = keras.layers.concatenate([latent_input, BC_input])\n",
    "\n",
    "#fully connected layer 1:\n",
    "x = Dense(500, kernel_regularizer = regularizers.l2(l_decode))(x)\n",
    "x = BatchNormalization(axis = 1)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#fully connected layer 2:\n",
    "x = Dense((shape[1]-1)*(shape[2]-1)*shape[3], kernel_regularizer = regularizers.l2(l_decode))(x)\n",
    "x = BatchNormalization(axis = 1)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#reshaping:\n",
    "x = Reshape((shape[1]-1, shape[2]-1, shape[3]))(x)\n",
    "\n",
    "#CNN/upsampling layer 1:\n",
    "x = Conv2DTranspose(4, (3, 3), padding='same', kernel_regularizer = regularizers.l2(l_decode))(x)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "#CNN/upsampling layer 2:\n",
    "x = Conv2DTranspose(2, (3, 3), padding='same', kernel_regularizer = regularizers.l2(l_decode))(x)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "#output:\n",
    "outputs = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same', name='bathymetry')(x)\n",
    "\n",
    "#set decoder model:\n",
    "decoder = Model([latent_input,BC_input], outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_outputs = decoder([encoder(bathy_input)[2],BC_input])\n",
    "vae = Model([bathy_input,BC_input], vae_outputs, name='vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vae_loss(y_true, y_pred):\n",
    "    #KL coefficient:\n",
    "    l_kl=1.0 \n",
    "    \n",
    "    #mse loss:\n",
    "    mse_loss = mse(K.flatten(y_true), K.flatten(y_pred))\n",
    "    \n",
    "    #KL loss:\n",
    "    kl_loss = - 0.5 * K.mean(1.0 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    \n",
    "    #total loss\n",
    "    vae_loss = mse_loss +l_kl*kl_loss\n",
    "    return vae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.compile(optimizer='Adam', loss=vae_loss)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_dummy = Z.reshape(41,501,-1) \n",
    "Z_new = np.zeros((Z_dummy.shape[2],Z_dummy.shape[0],Z_dummy.shape[1])) \n",
    "for i in range(Z_dummy.shape[2]):\n",
    "    Z_new[i,:,:] = Z_dummy[:,:,i]     \n",
    "del Z_dummy\n",
    "\n",
    "#check Z_new shape:\n",
    "print(Z_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_dummy = vx.reshape(41,501,-1)\n",
    "vx_new = np.zeros((vx_dummy.shape[2],vx_dummy.shape[0],vx_dummy.shape[1]))\n",
    "for i in range(vx_dummy.shape[2]):\n",
    "    vx_new[i,:,:] = vx_dummy[:,:,i]\n",
    "del vx_dummy\n",
    "\n",
    "#check vx_new shape:\n",
    "print(vx_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "vy_dummy = vy.reshape(41,501,-1)\n",
    "vy_new = np.zeros((vy_dummy.shape[2],vy_dummy.shape[0],vy_dummy.shape[1]))\n",
    "for i in range(vy_dummy.shape[2]):\n",
    "    vy_new[i,:,:] = vy_dummy[:,:,i]\n",
    "del vy_dummy\n",
    "\n",
    "#check vy_new shape:\n",
    "print(vy_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add channel dimension:\n",
    "\n",
    "y_train = Z_new.reshape(-1,41,501,1) #output (Z)\n",
    "x_train = vx_new.reshape(-1,41,501,1) #input (vx; can be replaced with vy or both vx/vy)\n",
    "\n",
    "#check shapes:\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find normalizing parameters:\n",
    "\n",
    "x_min = np.min(x_train, axis=0)\n",
    "y_min = np.min(y_train, axis=0)\n",
    "x_range = (np.max(x_train, axis=0)-np.min(x_train, axis=0))\n",
    "y_range = (np.max(y_train, axis=0)-np.min(y_train, axis=0))\n",
    "\n",
    "#check shapes:\n",
    "print(x_min.shape)\n",
    "print(y_min.shape)\n",
    "print(x_range.shape)\n",
    "print(y_range.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize input/outputs:\n",
    "\n",
    "x_train_norm = (x_train-x_min)/x_range \n",
    "y_train_norm = (y_train-y_min)/y_range \n",
    "\n",
    "#check shapes:\n",
    "print(x_train_norm.shape)\n",
    "print(y_train_norm.shape)\n",
    "\n",
    "#check min/max:\n",
    "print(np.min(x_train_norm))\n",
    "print(np.min(y_train_norm))\n",
    "print(np.max(x_train_norm))\n",
    "print(np.max(y_train_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize BC (ensure it's between 0 and 1):\n",
    "\n",
    "BC = np.zeros((2, z_f.shape[1]))\n",
    "BC[0,:] = z_f/120\n",
    "BC[1,:] = Q_b/10000\n",
    "\n",
    "#check shapes and maxes:\n",
    "print(BC.shape)\n",
    "print(max(BC[0,:]))\n",
    "print(max(BC[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation:\n",
    "\n",
    "#generate a seed number for reproducibility:\n",
    "seed_num=np.random.randint(0,1e5)\n",
    "#seed_num=0 #use for reproducing a previously known seed #\n",
    "print(seed_num)\n",
    "\n",
    "#generate seed:\n",
    "np.random.seed(seed_num)\n",
    "\n",
    "#shuffle based on the seed:\n",
    "indx=np.arange(x_train_norm.shape[0]) #original indices (ascending) \n",
    "indx_new=copy.deepcopy(indx) #new, to be shuffled, indices\n",
    "np.random.shuffle(indx_new) #shuffled idices\n",
    "\n",
    "#create shuffled input/outputs:\n",
    "x_train_norm_shfl=copy.deepcopy(x_train_norm)\n",
    "y_train_norm_shfl=copy.deepcopy(y_train_norm)\n",
    "BC_shfl=copy.deepcopy(BC)\n",
    "for i in range(x_train_norm.shape[0]):\n",
    "    x_train_norm_shfl[i,:,:,:]=x_train_norm[indx_new[i],:,:,:]\n",
    "    y_train_norm_shfl[i,:,:,:]=y_train_norm[indx_new[i],:,:,:]\n",
    "    BC_shfl[:,i]=BC[:,indx_new[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=700 # number of data to be used\n",
    "val_split=0.2 #validation split\n",
    "\n",
    "#vae.load_weights('vae_i_m700_2_4_B_500_50_l0_1_0.h5') #to load saved weights, if necessary\n",
    "vae.fit(x={'velocity':x_train_norm_shfl[:N,:,:,:], 'BC':BC_shfl[:,:N].T},y=y_train_norm_shfl[:N,:40,:500,:],\n",
    "        epochs=100,batch_size=32,shuffle=False,validation_split=val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-soviet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
